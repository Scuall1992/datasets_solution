{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am creating a spark session object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "sc = SparkContext(appName=\"MyApp2\")\n",
    "spark = (\n",
    "    SparkSession.builder \n",
    "    .master('local[*]')\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have extracted a common methods to clean up the datasets\n",
    "\n",
    "split_by_symbol - this is for split the catefories by specific symbol (, or & in our cases)\n",
    "\n",
    "lower_and_trim and add_prefix_to_col are iterating throught all columns\n",
    "\n",
    "clean_phone - remove all symbols except a digits from a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def split_by_symbol(df: DataFrame, sym: str, column: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Splits the values in the specified column of a DataFrame by a given symbol and explodes them into separate rows.\n",
    "    \n",
    "    :param df: Input DataFrame\n",
    "    :param sym: Symbol to split by\n",
    "    :param column: Column in which to look for the symbol\n",
    "    :return: DataFrame with values in the specified column split by the symbol\n",
    "    \"\"\"\n",
    "    df_no = df.filter(~F.col(column).contains(sym) | F.col(column).isNull())\n",
    "    df_yes = df.filter(F.col(column).contains(sym))\n",
    "    res = df_yes.withColumn(\n",
    "        column, \n",
    "        F.explode(\n",
    "            F.split(df_yes[column], sym)\n",
    "        )\n",
    "    )\n",
    "    return res.distinct().unionByName(df_no)\n",
    "\n",
    "def lower_and_trim(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Converts all the column values of a DataFrame to lowercase and trims any spaces.\n",
    "    \n",
    "    :param df: Input DataFrame\n",
    "    :return: DataFrame with all values in lowercase and trimmed\n",
    "    \"\"\"\n",
    "    return df.select([F.trim(F.lower(F.col(column))).alias(column) for column in df.columns])\n",
    "\n",
    "def add_prefix_to_cols(df: DataFrame, prefix: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Adds a prefix to all the column names of a DataFrame.\n",
    "    \n",
    "    :param df: Input DataFrame\n",
    "    :param prefix: Prefix string to be added\n",
    "    :return: DataFrame with the prefixed column names\n",
    "    \"\"\"\n",
    "    return df.select([F.col(column).alias(f\"{prefix}_{column}\") for column in df.columns])\n",
    "\n",
    "def clean_phone(df: DataFrame, column: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans phone number data by removing all non-numeric characters from a specified column in a DataFrame.\n",
    "    \n",
    "    :param df: Input DataFrame\n",
    "    :param column: Column containing phone numbers to be cleaned\n",
    "    :return: DataFrame with cleaned phone numbers\n",
    "    \"\"\"\n",
    "    return df.withColumn(column, F.regexp_replace(column, \"[^0-9]\", \"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'm reading the datasets and PySpark can easilly read the facebook dataset even with the errors in csv markup\n",
    "\n",
    "Pandas couldn't do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_df = spark.read.csv(\"data/facebook_dataset.csv\", header=True)\n",
    "gg_df = spark.read.csv(\"data/google_dataset.csv\", header=True)\n",
    "wb_df = spark.read.csv(\"data/website_dataset.csv\", header=True, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_df.show(20, truncate=False)\n",
    "gg_df.show(20, truncate=False)\n",
    "wb_df.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'm selecting only needed columns and rename similar through all datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restructure and rename columns \n",
    "\n",
    "fb_df = fb_df.select(\n",
    "    \"domain\",\n",
    "    \"address\",\n",
    "    \"categories\",\n",
    "    \"city\",\n",
    "    \"country_code\",\n",
    "    \"country_name\",\n",
    "    \"name\",\n",
    "    \"phone\",\n",
    "    \"region_code\",\n",
    "    \"region_name\",\n",
    "    \"zip_code\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "gg_df = gg_df.select(\n",
    "    \"domain\",\n",
    "    \"address\",\n",
    "    \"category\",\n",
    "    \"city\",\n",
    "    \"country_code\",\n",
    "    \"country_name\",\n",
    "    \"name\",\n",
    "    \"phone\",\n",
    "    \"region_code\",\n",
    "    \"region_name\",\n",
    "    \"zip_code\",\n",
    "    \"raw_address\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "wb_df = wb_df.select(\n",
    "    F.col(\"root_domain\").alias(\"domain\"),\n",
    "    F.col(\"s_category\").alias(\"category\"),\n",
    "    F.col(\"main_city\").alias(\"city\"),\n",
    "    F.col(\"legal_name\").alias(\"name\"),\n",
    "    F.col(\"main_region\").alias(\"region_name\"),\n",
    "    F.col(\"phone\"),\n",
    "    F.col(\"site_name\"),\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we start the cleaning. I have decided to denormalise data and split categories.\n",
    "\n",
    "So we have increased a dataset size from 72077 to 145232"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean facebook\n",
    "\n",
    "from pyspark.sql.functions import explode, split\n",
    "\n",
    "#Split categories by |\n",
    "\n",
    "fb_df_null_cat = fb_df.filter(F.col(\"categories\").isNull()).withColumnRenamed(\"categories\", \"category\")\n",
    "fb_df_cat = fb_df.filter(F.col(\"categories\").isNotNull())\n",
    "\n",
    "fb_df_exploded = fb_df_cat.withColumn(\n",
    "    \"category\", \n",
    "    explode(\n",
    "        split(fb_df[\"categories\"], \"\\|\")\n",
    "    )\n",
    ").drop(\"categories\")\n",
    "\n",
    "\n",
    "print(fb_df.count())\n",
    "print(fb_df_exploded.distinct().unionByName(fb_df_null_cat).count())\n",
    "\n",
    "#Split category by &\n",
    "\n",
    "fb_df_exploded_amp = split_by_symbol(fb_df_exploded, \"&\", \"category\")\n",
    "\n",
    "print(fb_df_exploded_amp.count())\n",
    "\n",
    "#Split category by ,\n",
    "\n",
    "fb_df_exploded_amp = split_by_symbol(fb_df_exploded_amp, \",\", \"category\")\n",
    "\n",
    "print(fb_df_exploded_amp.count())\n",
    "\n",
    "#Clean phone numbers\n",
    "\n",
    "fb_df_exploded_amp_phone = clean_phone(fb_df_exploded_amp, \"phone\")\n",
    "\n",
    "#Lower and trim all columns\n",
    "#Rename cols\n",
    "\n",
    "fb_df_clean = add_prefix_to_cols(lower_and_trim(fb_df_exploded_amp_phone), \"fb\")\n",
    "\n",
    "fb_df_clean.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have increased a dataset size from 356520 to 580891"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean google\n",
    "\n",
    "\n",
    "#Split category by &\n",
    "\n",
    "print(gg_df.count())\n",
    "\n",
    "gg_df_amp_exploded = split_by_symbol(gg_df, \"&\", \"category\")\n",
    "\n",
    "print(gg_df_amp_exploded.count())\n",
    "\n",
    "#Split category by ,\n",
    "gg_df_amp_exploded = split_by_symbol(gg_df_amp_exploded, \",\", \"category\")\n",
    "\n",
    "print(gg_df_amp_exploded.count())\n",
    "\n",
    "#Clean phone numbers\n",
    "\n",
    "gg_df_amp_exploded_phone = clean_phone(gg_df_amp_exploded, \"phone\")\n",
    "\n",
    "#Lower and trim all columns\n",
    "#Rename cols\n",
    "\n",
    "gg_df_clean = add_prefix_to_cols(lower_and_trim(gg_df_amp_exploded_phone), \"gg\")\n",
    "\n",
    "gg_df_clean.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean website\n",
    "\n",
    "#Split category by &\n",
    "\n",
    "print(wb_df.count())\n",
    "\n",
    "wb_df_amp_exploded = split_by_symbol(wb_df, \"&\", \"category\")\n",
    "\n",
    "print(wb_df_amp_exploded.count())\n",
    "\n",
    "#Split category by ,\n",
    "wb_df_amp_exploded = split_by_symbol(wb_df_amp_exploded, \",\", \"category\")\n",
    "\n",
    "print(wb_df_amp_exploded.count())\n",
    "\n",
    "#Clean phone numbers\n",
    "\n",
    "wb_df_amp_exploded_phone = clean_phone(wb_df_amp_exploded, \"phone\")\n",
    "\n",
    "#Lower and trim all columns\n",
    "#Rename cols\n",
    "wb_df_clean = add_prefix_to_cols(lower_and_trim(wb_df_amp_exploded_phone), \"wb\")\n",
    "\n",
    "wb_df_clean.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition I'm also remove all rows where name is null  \n",
    "\n",
    "... rows was filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"With null names \", fb_df_clean.count())\n",
    "print(\"With null names \", gg_df_clean.count())\n",
    "print(\"With null names \", wb_df_clean.count())\n",
    "\n",
    "fb_df_clean_ = fb_df_clean.filter(F.col(\"fb_name\").isNotNull())\n",
    "gg_df_clean_ = gg_df_clean.filter(F.col(\"gg_name\").isNotNull())\n",
    "wb_df_clean_ = wb_df_clean.filter(F.col(\"wb_name\").isNotNull())\n",
    "\n",
    "print(\"Without null names \", fb_df_clean_.count())\n",
    "print(\"Without null names \", gg_df_clean_.count())\n",
    "print(\"Without null names \", wb_df_clean_.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'm converting the datasets to .parquet format, and now it become less on disk\n",
    "\n",
    "Before 153MB\n",
    "After 66MB\n",
    "\n",
    "And spark works faster with .parquet format, rather than csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_df_clean_.write.mode(\"overwrite\").parquet(\"fb_df\")\n",
    "gg_df_clean_.write.mode(\"overwrite\").parquet(\"gg_df\")\n",
    "wb_df_clean_.write.mode(\"overwrite\").parquet(\"wb_df\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
